{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Running list of issues\n",
    "\n",
    "# can't seem to write the full raw event to hdfs (probably because how set up the schema)\n",
    "# respect different between timestamp and event time - for knowing when to handle watermarks\n",
    "# not really sure what the windowing is telling us or how might be used optimally in analysis, would need to join with fill denormalized tables?\n",
    "# issues with writing to kafka stream\n",
    "# add vandalism as topic: \"vandalism\" in comment\n",
    "# capture diffs: https://en.wikipedia.org/w/index.php?title=1828_in_the_United_States&diff=827995885&oldid=827995860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prune_event(df_IN, filter_IN):\n",
    "    df_OUT = df_IN.select(\"parsed_wiki_values.*\") \\\n",
    "            .select(\"id\", \\\n",
    "                    \"user\", \\\n",
    "                    \"timestamp\", \\\n",
    "                    \"bot\", \\\n",
    "                    \"comment\", \\\n",
    "                    \"server_name\", \\\n",
    "                    \"wiki\", \\\n",
    "                    \"title\", \\\n",
    "                    \"type\", \\\n",
    "                    \"log_action\", \\\n",
    "                    \"log_action_comment\", \\\n",
    "                    \"log_type\", \\\n",
    "                    \"minor\", \\\n",
    "                    \"namespace\", \\\n",
    "                    \"parsedcomment\", \\\n",
    "                    \"patrolled\", \\\n",
    "                    col(\"meta.dt\").alias(\"event_date\"), \\\n",
    "                    col(\"meta.schema_uri\").alias(\"event_schema_uri\"), \\\n",
    "                    col(\"meta.uri\").alias(\"wikipage_uri\"), \\\n",
    "                    col(\"meta.domain\").alias(\"event_domain\"), \\\n",
    "                    col(\"length.old\").alias(\"len_old\"), \\\n",
    "                    col(\"length.new\").alias(\"len_new\"), \\\n",
    "                    col(\"revision.old\").alias(\"rev_old\"), \\\n",
    "                    col(\"revision.new\").alias(\"rev_new\"), \\\n",
    "                   ).where(col(\"server_name\")==filter_IN)\n",
    "    return df_OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_hdfs(stream, location):\n",
    "    pathout = \"hdfs://sandbox.hortonworks.com:8020/tmp/{}\".format(location)\n",
    "    return stream.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"path\", pathout) \\\n",
    "    .option(\"checkpointLocation\", pathout) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def windowed_counts(df_IN, col_IN):\n",
    "    return df_IN.groupBy(\n",
    "        window(df_IN[col_IN], \"1 minutes\", \"30 seconds\"),\n",
    "        df_IN.user\n",
    "    ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_anonymous(userid):\n",
    "    '''It is assumed in wikipedia that anonymous users are\n",
    "       given the userid of the IP address from which their\n",
    "       traffic is coming from. IPs can be ipv4 or ipv6'''\n",
    "    \n",
    "    ANONYMOUS = False\n",
    "    \n",
    "    # ipv4 and ipv6 expressions used from: http://nbviewer.jupyter.org/github/rasbt/python_reference/blob/master/tutorials/useful_regex.ipynb\n",
    "    ipv4_pattern = r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$'\n",
    "    ipv6_pattern = r'^\\s*((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?\\s*$'\n",
    "    ipv4_ans = bool(re.match(ipv4_pattern, userid))\n",
    "    ipv6_ans = bool(re.match(ipv6_pattern, userid))\n",
    "    if ipv4_ans is True:\n",
    "        ANONYMOUS = True\n",
    "    elif ipv6_ans is True:\n",
    "        ANONYMOUS = True\n",
    "    else:\n",
    "        ANONYMOUS = False\n",
    "    return ANONYMOUS\n",
    "\n",
    "matchAnonUDF = udf(match_anonymous, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\") \\\n",
    "                     .option(\"kafka.bootstrap.servers\",\"sandbox.hortonworks.com:6667\") \\\n",
    "                     .option(\"subscribe\", \"wiki-rc-stream\") \\\n",
    "                     .option(\"startingOffsets\", \"earliest\") \\\n",
    "                     .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonschema = StructType().add(\"bot\", BooleanType()) \\\n",
    "                         .add(\"comment\", StringType()) \\\n",
    "                         .add(\"id\", IntegerType()) \\\n",
    "                         .add(\"length\", StructType() \\\n",
    "                            .add(\"new\", IntegerType()) \\\n",
    "                            .add(\"old\", IntegerType())) \\\n",
    "                         .add(\"meta\", StructType() \\\n",
    "                            .add(\"domain\", StringType()) \\\n",
    "                            .add(\"dt\", StringType()) \\\n",
    "                            .add(\"id\", StringType()) \\\n",
    "                            .add(\"request_id\", StringType()) \\\n",
    "                            .add(\"schema_uri\", StringType()) \\\n",
    "                            .add(\"topic\", StringType()) \\\n",
    "                            .add(\"partition\", IntegerType()) \\\n",
    "                            .add(\"uri\", StringType()) \\\n",
    "                            .add(\"offset\", IntegerType())) \\\n",
    "                         .add(\"minor\",  BooleanType()) \\\n",
    "                         .add(\"namespace\", IntegerType()) \\\n",
    "                         .add(\"parsedcomment\", StringType()) \\\n",
    "                         .add(\"patrolled\", BooleanType()) \\\n",
    "                         .add(\"revision\", StructType() \\\n",
    "                            .add(\"new\", IntegerType()) \\\n",
    "                            .add(\"old\", IntegerType())) \\\n",
    "                         .add(\"server_name\", StringType()) \\\n",
    "                         .add(\"server_script_path\", StringType()) \\\n",
    "                         .add(\"server_url\", StringType()) \\\n",
    "                         .add(\"timestamp\", StringType()) \\\n",
    "                         .add(\"title\", StringType()) \\\n",
    "                         .add(\"type\", StringType()) \\\n",
    "                         .add(\"user\", StringType()) \\\n",
    "                         .add(\"wiki\", StringType()) \\\n",
    "                         .add(\"log_action\", StringType()) \\\n",
    "                         .add(\"log_action_comment\", StringType()) \\\n",
    "                         .add(\"log_id\", IntegerType()) \\\n",
    "                         .add(\"log_params\", StructType()) \\\n",
    "                         .add(\"log_type\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_raw = df.select(from_json(col(\"value\") \\\n",
    "                                .cast(\"string\"), jsonschema) \\\n",
    "                                .alias(\"parsed_wiki_values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[parsed_wiki_values: struct<bot:boolean,comment:string,id:int,length:struct<new:int,old:int>,meta:struct<domain:string,dt:string,id:string,request_id:string,schema_uri:string,topic:string,partition:int,uri:string,offset:int>,minor:boolean,namespace:int,parsedcomment:string,patrolled:boolean,revision:struct<new:int,old:int>,server_name:string,server_script_path:string,server_url:string,timestamp:string,title:string,type:string,user:string,wiki:string,log_action:string,log_action_comment:string,log_id:int,log_params:struct<>,log_type:string>]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_raw.printSchema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_wiki = prune_event(wiki_raw, \"en.wikipedia.org\") \\\n",
    "                        .withColumn(\"anonymous\", matchAnonUDF(\"user\")) \\\n",
    "                        .withColumn(\"timestamp_dt\", from_unixtime(\"timestamp\", \"yyyy-MM-dd HH:mm:ss.SSS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[id: int, user: string, timestamp: string, bot: boolean, comment: string, server_name: string, wiki: string, title: string, type: string, log_action: string, log_action_comment: string, log_type: string, minor: boolean, namespace: int, parsedcomment: string, patrolled: boolean, event_date: string, event_schema_uri: string, wikipage_uri: string, event_domain: string, len_old: int, len_new: int, rev_old: int, rev_new: int, anonymous: boolean, timestamp_dt: string]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_wiki.printSchema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikidata = prune_event(wiki_raw, \"www.wikidata.org\") \\\n",
    "                        .withColumn(\"anonymous\", matchAnonUDF(\"user\")) \\\n",
    "                        .withColumn(\"timestamp_dt\", from_unixtime(\"timestamp\", \"yyyy-MM-dd HH:mm:ss.SSS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[id: int, user: string, timestamp: string, bot: boolean, comment: string, server_name: string, wiki: string, title: string, type: string, log_action: string, log_action_comment: string, log_type: string, minor: boolean, namespace: int, parsedcomment: string, patrolled: boolean, event_date: string, event_schema_uri: string, wikipage_uri: string, event_domain: string, len_old: int, len_new: int, rev_old: int, rev_new: int, anonymous: boolean, timestamp_dt: string]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikidata.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query3 = en_wiki.writeStream.outputMode(\"append\").format(\"console\") \\\n",
    "                                             .start()\n",
    "query3.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query4 = wikidata.writeStream.outputMode(\"append\").format(\"console\") \\\n",
    "                                             .start()\n",
    "query4.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_en_wiki_hdfs = write_to_hdfs(en_wiki, \"en_wiki\")\n",
    "query_en_wiki_hdfs.awaitTermination(timeout=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_wikidata_hdfs = write_to_hdfs(wikidata, \"wikidata\")\n",
    "query_wikidata_hdfs.awaitTermination(timeout=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowedEnWikiCounts = windowed_counts(en_wiki, 'timestamp_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowedWikidataCounts = windowed_counts(wikidata, 'timestamp_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = windowedEnWikiCounts.writeStream.outputMode(\"complete\").format(\"console\") \\\n",
    "                                             .start()\n",
    "query5.awaitTermination(timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query6 = windowedWikidataCounts.writeStream.outputMode(\"complete\").format(\"console\") \\\n",
    "                                             .start()\n",
    "query6.awaitTermination(timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "'Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 4, localhost, executor driver): java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$3()Lscala/Option;\\n\\tat org.apache.spark.sql.kafka010.KafkaWriteTask.createProjection(KafkaWriteTask.scala:112)\\n\\tat org.apache.spark.sql.kafka010.KafkaWriteTask.<init>(KafkaWriteTask.scala:39)\\n\\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:90)\\n\\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:89)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\\nDriver stacktrace:\\n=== Streaming Query ===\\nIdentifier: [id = e3841b70-201b-4c79-8510-1ea71b52d4ce, runId = d743dfa0-657b-4f8e-9e21-619c2c292d50]\\nCurrent Committed Offsets: {}\\nCurrent Available Offsets: {KafkaSource[Subscribe[wiki-rc-stream]]: {\"wiki-rc-stream\":{\"2\":70541,\"1\":69974,\"0\":70011}}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nProject [cast(key#381 as string) AS key#386, cast(value#382 as string) AS value#387]\\n+- Project [structtojson(named_struct(user, user#39, window, window#311)) AS key#381, cast(count#340L as string) AS value#382]\\n   +- Aggregate [window#341, user#39], [window#341 AS window#311, user#39, count(1) AS count#340L]\\n      +- Filter ((cast(timestamp_dt#139 as timestamp) >= window#341.start) && (cast(timestamp_dt#139 as timestamp) < window#341.end))\\n         +- Expand [ArrayBuffer(named_struct(start, ((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0), end, (((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000)), id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, anonymous#112, timestamp_dt#139), ArrayBuffer(named_struct(start, ((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0), end, (((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000)), id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, anonymous#112, timestamp_dt#139), ArrayBuffer(named_struct(start, ((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(2 as bigint)) - cast(2 as bigint)) * 30000000) + 0), end, (((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(2 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000)), id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, anonymous#112, timestamp_dt#139)], [window#341, id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, ... 3 more fields]\\n            +- Project [id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, ... 2 more fields]\\n               +- Project [id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, match_anonymous(user#39) AS anonymous#112]\\n                  +- Filter (server_name#33 = en.wikipedia.org)\\n                     +- Project [id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, meta#27.dt AS event_date#70, meta#27.schema_uri AS event_schema_uri#71, meta#27.uri AS wikipage_uri#72, meta#27.domain AS event_domain#73, length#26.old AS len_old#74, length#26.new AS len_new#75, revision#32.old AS rev_old#76, revision#32.new AS rev_new#77]\\n                        +- Project [parsed_wiki_values#20.bot AS bot#23, parsed_wiki_values#20.comment AS comment#24, parsed_wiki_values#20.id AS id#25, parsed_wiki_values#20.length AS length#26, parsed_wiki_values#20.meta AS meta#27, parsed_wiki_values#20.minor AS minor#28, parsed_wiki_values#20.namespace AS namespace#29, parsed_wiki_values#20.parsedcomment AS parsedcomment#30, parsed_wiki_values#20.patrolled AS patrolled#31, parsed_wiki_values#20.revision AS revision#32, parsed_wiki_values#20.server_name AS server_name#33, parsed_wiki_values#20.server_script_path AS server_script_path#34, parsed_wiki_values#20.server_url AS server_url#35, parsed_wiki_values#20.timestamp AS timestamp#36, parsed_wiki_values#20.title AS title#37, parsed_wiki_values#20.type AS type#38, parsed_wiki_values#20.user AS user#39, parsed_wiki_values#20.wiki AS wiki#40, parsed_wiki_values#20.log_action AS log_action#41, parsed_wiki_values#20.log_action_comment AS log_action_comment#42, parsed_wiki_values#20.log_id AS log_id#43, parsed_wiki_values#20.log_params AS log_params#44, parsed_wiki_values#20.log_type AS log_type#45]\\n                           +- Project [jsontostruct(StructField(bot,BooleanType,true), StructField(comment,StringType,true), StructField(id,IntegerType,true), StructField(length,StructType(StructField(new,IntegerType,true), StructField(old,IntegerType,true)),true), StructField(meta,StructType(StructField(domain,StringType,true), StructField(dt,StringType,true), StructField(id,StringType,true), StructField(request_id,StringType,true), StructField(schema_uri,StringType,true), StructField(topic,StringType,true), StructField(partition,IntegerType,true), StructField(uri,StringType,true), StructField(offset,IntegerType,true)),true), StructField(minor,BooleanType,true), StructField(namespace,IntegerType,true), StructField(parsedcomment,StringType,true), StructField(patrolled,BooleanType,true), StructField(revision,StructType(StructField(new,IntegerType,true), StructField(old,IntegerType,true)),true), StructField(server_name,StringType,true), StructField(server_script_path,StringType,true), StructField(server_url,StringType,true), StructField(timestamp,StringType,true), StructField(title,StringType,true), StructField(type,StringType,true), StructField(user,StringType,true), StructField(wiki,StringType,true), StructField(log_action,StringType,true), StructField(log_action_comment,StringType,true), StructField(log_id,IntegerType,true), StructField(log_params,StructType(),true), StructField(log_type,StringType,true), cast(value#1 as string)) AS parsed_wiki_values#20]\\n                              +- StreamingExecutionRelation KafkaSource[Subscribe[wiki-rc-stream]], [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o210.awaitTermination.\n: org.apache.spark.sql.streaming.StreamingQueryException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 4, localhost, executor driver): java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$3()Lscala/Option;\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.createProjection(KafkaWriteTask.scala:112)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.<init>(KafkaWriteTask.scala:39)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:90)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n=== Streaming Query ===\nIdentifier: [id = e3841b70-201b-4c79-8510-1ea71b52d4ce, runId = d743dfa0-657b-4f8e-9e21-619c2c292d50]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaSource[Subscribe[wiki-rc-stream]]: {\"wiki-rc-stream\":{\"2\":70541,\"1\":69974,\"0\":70011}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [cast(key#381 as string) AS key#386, cast(value#382 as string) AS value#387]\n+- Project [structtojson(named_struct(user, user#39, window, window#311)) AS key#381, cast(count#340L as string) AS value#382]\n   +- Aggregate [window#341, user#39], [window#341 AS window#311, user#39, count(1) AS count#340L]\n      +- Filter ((cast(timestamp_dt#139 as timestamp) >= window#341.start) && (cast(timestamp_dt#139 as timestamp) < window#341.end))\n         +- Expand [ArrayBuffer(named_struct(start, ((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0), end, (((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000)), id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, anonymous#112, timestamp_dt#139), ArrayBuffer(named_struct(start, ((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0), end, (((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000)), id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, anonymous#112, timestamp_dt#139), ArrayBuffer(named_struct(start, ((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(2 as bigint)) - cast(2 as bigint)) * 30000000) + 0), end, (((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(2 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000)), id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, anonymous#112, timestamp_dt#139)], [window#341, id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, ... 3 more fields]\n            +- Project [id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, ... 2 more fields]\n               +- Project [id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, match_anonymous(user#39) AS anonymous#112]\n                  +- Filter (server_name#33 = en.wikipedia.org)\n                     +- Project [id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, meta#27.dt AS event_date#70, meta#27.schema_uri AS event_schema_uri#71, meta#27.uri AS wikipage_uri#72, meta#27.domain AS event_domain#73, length#26.old AS len_old#74, length#26.new AS len_new#75, revision#32.old AS rev_old#76, revision#32.new AS rev_new#77]\n                        +- Project [parsed_wiki_values#20.bot AS bot#23, parsed_wiki_values#20.comment AS comment#24, parsed_wiki_values#20.id AS id#25, parsed_wiki_values#20.length AS length#26, parsed_wiki_values#20.meta AS meta#27, parsed_wiki_values#20.minor AS minor#28, parsed_wiki_values#20.namespace AS namespace#29, parsed_wiki_values#20.parsedcomment AS parsedcomment#30, parsed_wiki_values#20.patrolled AS patrolled#31, parsed_wiki_values#20.revision AS revision#32, parsed_wiki_values#20.server_name AS server_name#33, parsed_wiki_values#20.server_script_path AS server_script_path#34, parsed_wiki_values#20.server_url AS server_url#35, parsed_wiki_values#20.timestamp AS timestamp#36, parsed_wiki_values#20.title AS title#37, parsed_wiki_values#20.type AS type#38, parsed_wiki_values#20.user AS user#39, parsed_wiki_values#20.wiki AS wiki#40, parsed_wiki_values#20.log_action AS log_action#41, parsed_wiki_values#20.log_action_comment AS log_action_comment#42, parsed_wiki_values#20.log_id AS log_id#43, parsed_wiki_values#20.log_params AS log_params#44, parsed_wiki_values#20.log_type AS log_type#45]\n                           +- Project [jsontostruct(StructField(bot,BooleanType,true), StructField(comment,StringType,true), StructField(id,IntegerType,true), StructField(length,StructType(StructField(new,IntegerType,true), StructField(old,IntegerType,true)),true), StructField(meta,StructType(StructField(domain,StringType,true), StructField(dt,StringType,true), StructField(id,StringType,true), StructField(request_id,StringType,true), StructField(schema_uri,StringType,true), StructField(topic,StringType,true), StructField(partition,IntegerType,true), StructField(uri,StringType,true), StructField(offset,IntegerType,true)),true), StructField(minor,BooleanType,true), StructField(namespace,IntegerType,true), StructField(parsedcomment,StringType,true), StructField(patrolled,BooleanType,true), StructField(revision,StructType(StructField(new,IntegerType,true), StructField(old,IntegerType,true)),true), StructField(server_name,StringType,true), StructField(server_script_path,StringType,true), StructField(server_url,StringType,true), StructField(timestamp,StringType,true), StructField(title,StringType,true), StructField(type,StringType,true), StructField(user,StringType,true), StructField(wiki,StringType,true), StructField(log_action,StringType,true), StructField(log_action_comment,StringType,true), StructField(log_id,IntegerType,true), StructField(log_params,StructType(),true), StructField(log_type,StringType,true), cast(value#1 as string)) AS parsed_wiki_values#20]\n                              +- StreamingExecutionRelation KafkaSource[Subscribe[wiki-rc-stream]], [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:305)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:191)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 4, localhost, executor driver): java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$3()Lscala/Option;\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.createProjection(KafkaWriteTask.scala:112)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.<init>(KafkaWriteTask.scala:39)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:90)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1.apply$mcV$sp(KafkaWriter.scala:89)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1.apply(KafkaWriter.scala:89)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1.apply(KafkaWriter.scala:89)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$.write(KafkaWriter.scala:88)\n\tat org.apache.spark.sql.kafka010.KafkaSink.addBatch(KafkaSink.scala:38)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1.apply$mcV$sp(StreamExecution.scala:554)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1.apply(StreamExecution.scala:554)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1.apply(StreamExecution.scala:554)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:278)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:49)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(StreamExecution.scala:553)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply$mcV$sp(StreamExecution.scala:273)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:262)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1$$anonfun$1.apply(StreamExecution.scala:262)\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter$class.reportTimeTaken(ProgressReporter.scala:278)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:49)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1.apply$mcZ$sp(StreamExecution.scala:262)\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:43)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:257)\n\t... 1 more\nCaused by: java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$3()Lscala/Option;\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.createProjection(KafkaWriteTask.scala:112)\n\tat org.apache.spark.sql.kafka010.KafkaWriteTask.<init>(KafkaWriteTask.scala:39)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:90)\n\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-32296d36ba59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mstreamQuery1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timeout must be a positive integer or float. Got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.execution.QueryExecutionException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: 'Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 4, localhost, executor driver): java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.expressions.Cast$.apply$default$3()Lscala/Option;\\n\\tat org.apache.spark.sql.kafka010.KafkaWriteTask.createProjection(KafkaWriteTask.scala:112)\\n\\tat org.apache.spark.sql.kafka010.KafkaWriteTask.<init>(KafkaWriteTask.scala:39)\\n\\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:90)\\n\\tat org.apache.spark.sql.kafka010.KafkaWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(KafkaWriter.scala:89)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\\n\\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\\nDriver stacktrace:\\n=== Streaming Query ===\\nIdentifier: [id = e3841b70-201b-4c79-8510-1ea71b52d4ce, runId = d743dfa0-657b-4f8e-9e21-619c2c292d50]\\nCurrent Committed Offsets: {}\\nCurrent Available Offsets: {KafkaSource[Subscribe[wiki-rc-stream]]: {\"wiki-rc-stream\":{\"2\":70541,\"1\":69974,\"0\":70011}}}\\n\\nCurrent State: ACTIVE\\nThread State: RUNNABLE\\n\\nLogical Plan:\\nProject [cast(key#381 as string) AS key#386, cast(value#382 as string) AS value#387]\\n+- Project [structtojson(named_struct(user, user#39, window, window#311)) AS key#381, cast(count#340L as string) AS value#382]\\n   +- Aggregate [window#341, user#39], [window#341 AS window#311, user#39, count(1) AS count#340L]\\n      +- Filter ((cast(timestamp_dt#139 as timestamp) >= window#341.start) && (cast(timestamp_dt#139 as timestamp) < window#341.end))\\n         +- Expand [ArrayBuffer(named_struct(start, ((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0), end, (((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(0 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000)), id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, anonymous#112, timestamp_dt#139), ArrayBuffer(named_struct(start, ((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0), end, (((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(1 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000)), id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, anonymous#112, timestamp_dt#139), ArrayBuffer(named_struct(start, ((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(2 as bigint)) - cast(2 as bigint)) * 30000000) + 0), end, (((((CEIL((cast((precisetimestamp(cast(timestamp_dt#139 as timestamp)) - 0) as double) / cast(30000000 as double))) + cast(2 as bigint)) - cast(2 as bigint)) * 30000000) + 0) + 60000000)), id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, anonymous#112, timestamp_dt#139)], [window#341, id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, ... 3 more fields]\\n            +- Project [id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, ... 2 more fields]\\n               +- Project [id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, event_date#70, event_schema_uri#71, wikipage_uri#72, event_domain#73, len_old#74, len_new#75, rev_old#76, rev_new#77, match_anonymous(user#39) AS anonymous#112]\\n                  +- Filter (server_name#33 = en.wikipedia.org)\\n                     +- Project [id#25, user#39, timestamp#36, bot#23, comment#24, server_name#33, wiki#40, title#37, type#38, log_action#41, log_action_comment#42, log_type#45, minor#28, namespace#29, parsedcomment#30, patrolled#31, meta#27.dt AS event_date#70, meta#27.schema_uri AS event_schema_uri#71, meta#27.uri AS wikipage_uri#72, meta#27.domain AS event_domain#73, length#26.old AS len_old#74, length#26.new AS len_new#75, revision#32.old AS rev_old#76, revision#32.new AS rev_new#77]\\n                        +- Project [parsed_wiki_values#20.bot AS bot#23, parsed_wiki_values#20.comment AS comment#24, parsed_wiki_values#20.id AS id#25, parsed_wiki_values#20.length AS length#26, parsed_wiki_values#20.meta AS meta#27, parsed_wiki_values#20.minor AS minor#28, parsed_wiki_values#20.namespace AS namespace#29, parsed_wiki_values#20.parsedcomment AS parsedcomment#30, parsed_wiki_values#20.patrolled AS patrolled#31, parsed_wiki_values#20.revision AS revision#32, parsed_wiki_values#20.server_name AS server_name#33, parsed_wiki_values#20.server_script_path AS server_script_path#34, parsed_wiki_values#20.server_url AS server_url#35, parsed_wiki_values#20.timestamp AS timestamp#36, parsed_wiki_values#20.title AS title#37, parsed_wiki_values#20.type AS type#38, parsed_wiki_values#20.user AS user#39, parsed_wiki_values#20.wiki AS wiki#40, parsed_wiki_values#20.log_action AS log_action#41, parsed_wiki_values#20.log_action_comment AS log_action_comment#42, parsed_wiki_values#20.log_id AS log_id#43, parsed_wiki_values#20.log_params AS log_params#44, parsed_wiki_values#20.log_type AS log_type#45]\\n                           +- Project [jsontostruct(StructField(bot,BooleanType,true), StructField(comment,StringType,true), StructField(id,IntegerType,true), StructField(length,StructType(StructField(new,IntegerType,true), StructField(old,IntegerType,true)),true), StructField(meta,StructType(StructField(domain,StringType,true), StructField(dt,StringType,true), StructField(id,StringType,true), StructField(request_id,StringType,true), StructField(schema_uri,StringType,true), StructField(topic,StringType,true), StructField(partition,IntegerType,true), StructField(uri,StringType,true), StructField(offset,IntegerType,true)),true), StructField(minor,BooleanType,true), StructField(namespace,IntegerType,true), StructField(parsedcomment,StringType,true), StructField(patrolled,BooleanType,true), StructField(revision,StructType(StructField(new,IntegerType,true), StructField(old,IntegerType,true)),true), StructField(server_name,StringType,true), StructField(server_script_path,StringType,true), StructField(server_url,StringType,true), StructField(timestamp,StringType,true), StructField(title,StringType,true), StructField(type,StringType,true), StructField(user,StringType,true), StructField(wiki,StringType,true), StructField(log_action,StringType,true), StructField(log_action_comment,StringType,true), StructField(log_id,IntegerType,true), StructField(log_params,StructType(),true), StructField(log_type,StringType,true), cast(value#1 as string)) AS parsed_wiki_values#20]\\n                              +- StreamingExecutionRelation KafkaSource[Subscribe[wiki-rc-stream]], [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\\n'"
     ]
    }
   ],
   "source": [
    "# TODO: write to individual kafka streams (need other details?)\n",
    "# enWikiCounts\n",
    "# wikidataCounts\n",
    "# \n",
    "'''\n",
    "streamQuery1 = windowedEnWikiCounts.select(\n",
    "    col(\"user\").cast(\"string\").alias(\"key\"),\n",
    "    col(\"count\").cast(\"string\").alias(\"value\")) \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "    \n",
    "'''\n",
    "'''\n",
    "streamQuery1 = windowedEnWikiCounts.select(\n",
    "    to_json(struct(\"user\", \"window\")).alias(\"key\"),\n",
    "    col(\"count\").cast(\"string\").alias(\"value\")) \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "'''\n",
    "\n",
    "streamQuery1 = windowedEnWikiCounts.select(\n",
    "    to_json(struct(\"user\", \"window\")).alias(\"key\"),\n",
    "    col(\"count\").cast(\"string\").alias(\"value\")) \\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox.hortonworks.com:6667\") \\\n",
    "    .option(\"topic\", \"enWikiCounts\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://sandbox.hortonworks.com:8020/tmp/enWikiCounts\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "streamQuery1.awaitTermination(timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: write to a topic called en_wikipedia_hydrate to notify it's time to add onto that with raw change text and push to HDFS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
