{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Wikipedia Recent Changes Stream Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import re\n",
    "from bs4 import BeautifulSoup # \\ For parsing out contents of actual changes\n",
    "import requests               # /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1.2.6.1.0-129'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Functions to make tasks below more reusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prune_event(df_IN, filter_IN):\n",
    "    \n",
    "    '''Take in a dataframe of wikipedia recent changes and produce \n",
    "       only the properties we want going forward\n",
    "       \n",
    "       filter_IN is the wikipedia domain we want to filter on (e.g., en.wikipedia.org)\n",
    "    '''\n",
    "    df_OUT = df_IN.select(\"parsed_wiki_values.*\") \\\n",
    "            .select(\"id\", \\\n",
    "                    \"user\", \\\n",
    "                    \"timestamp\", \\\n",
    "                    \"bot\", \\\n",
    "                    \"comment\", \\\n",
    "                    \"server_name\", \\\n",
    "                    \"wiki\", \\\n",
    "                    \"title\", \\\n",
    "                    \"type\", \\\n",
    "                    \"log_action\", \\\n",
    "                    \"log_action_comment\", \\\n",
    "                    \"log_type\", \\\n",
    "                    \"minor\", \\\n",
    "                    \"namespace\", \\\n",
    "                    \"parsedcomment\", \\\n",
    "                    \"patrolled\", \\\n",
    "                    col(\"meta.dt\").alias(\"event_date\"), \\\n",
    "                    col(\"meta.schema_uri\").alias(\"event_schema_uri\"), \\\n",
    "                    col(\"meta.uri\").alias(\"wikipage_uri\"), \\\n",
    "                    col(\"meta.domain\").alias(\"event_domain\"), \\\n",
    "                    col(\"length.old\").alias(\"len_old\"), \\\n",
    "                    col(\"length.new\").alias(\"len_new\"), \\\n",
    "                    col(\"revision.old\").alias(\"rev_old\"), \\\n",
    "                    col(\"revision.new\").alias(\"rev_new\"), \\\n",
    "                   ).where(col(\"server_name\")==filter_IN)\n",
    "    return df_OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_hdfs(stream, location):\n",
    "    '''Provided a stream and a location in hdfs to write out to parquet file\n",
    "    '''\n",
    "    pathout = \"hdfs://sandbox.hortonworks.com:8020/tmp/{}\".format(location)\n",
    "    return stream.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"path\", pathout) \\\n",
    "    .option(\"checkpointLocation\", pathout) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def windowed_counts(df_IN, col_IN):\n",
    "    '''Give a data frame and a datetime column to create a window on.\n",
    "       only for non-bot users.\n",
    "       Window is set to 5 minutes for now with 2 minutes slide\n",
    "    '''\n",
    "    return df_IN.where(col(\"bot\")==False).groupBy(\n",
    "        window(df_IN[col_IN], \"5 minutes\", \"2 minutes\"),\n",
    "        df_IN.user\n",
    "    ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_anonymous(userid):\n",
    "    '''It is assumed in wikipedia that anonymous users are\n",
    "       given the userid of the IP address from which their\n",
    "       traffic is coming from. IPs can be ipv4 or ipv6\n",
    "       \n",
    "       This function checks to see whether a username is \n",
    "       an IP address and returns true if it is.\n",
    "       '''\n",
    "    \n",
    "    ANONYMOUS = False\n",
    "    \n",
    "    # ipv4 and ipv6 expressions used from: http://nbviewer.jupyter.org/github/rasbt/python_reference/blob/master/tutorials/useful_regex.ipynb\n",
    "    ipv4_pattern = r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$'\n",
    "    ipv6_pattern = r'^\\s*((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?\\s*$'\n",
    "    ipv4_ans = bool(re.match(ipv4_pattern, str(userid)))\n",
    "    ipv6_ans = bool(re.match(ipv6_pattern, str(userid)))\n",
    "    if ipv4_ans is True:\n",
    "        ANONYMOUS = True\n",
    "    elif ipv6_ans is True:\n",
    "        ANONYMOUS = True\n",
    "    else:\n",
    "        ANONYMOUS = False\n",
    "    return ANONYMOUS\n",
    "\n",
    "matchAnonUDF = udf(match_anonymous, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_raw_changes(old, curr):\n",
    "    text = None\n",
    "    url = \"https://en.wikipedia.org/w/index.php?diff={}&oldid={}\".format(str(curr), str(old))\n",
    "    xpath = 'table.diff'\n",
    "    res = requests.get(url)\n",
    "    if res.status_code == 200:\n",
    "        try:\n",
    "            soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "            diff = soup.select(xpath)\n",
    "            if diff is not []:\n",
    "                text = str(diff)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Could not grab changes from {}: {}\".format(url, e))\n",
    "    else:\n",
    "        print(\"Could not fetch resource at {} - status: {}\".format(url, str(res.status_code)))\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "rawChangesUDF = udf(grab_raw_changes, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in Raw Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\") \\\n",
    "                     .option(\"kafka.bootstrap.servers\",\"sandbox.hortonworks.com:6667\") \\\n",
    "                     .option(\"subscribe\", \"wiki-rc-stream\") \\\n",
    "                     .option(\"startingOffsets\", \"earliest\") \\\n",
    "                     .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Provide a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all properties will be in every event coming through the stream. Those that aren't present get a null. We are not concerned about making any properties non-nullable so we'll leave it as such for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonschema = StructType().add(\"bot\", BooleanType()) \\\n",
    "                         .add(\"comment\", StringType()) \\\n",
    "                         .add(\"id\", IntegerType()) \\\n",
    "                         .add(\"length\", StructType() \\\n",
    "                            .add(\"new\", IntegerType()) \\\n",
    "                            .add(\"old\", IntegerType())) \\\n",
    "                         .add(\"meta\", StructType() \\\n",
    "                            .add(\"domain\", StringType()) \\\n",
    "                            .add(\"dt\", StringType()) \\\n",
    "                            .add(\"id\", StringType()) \\\n",
    "                            .add(\"request_id\", StringType()) \\\n",
    "                            .add(\"schema_uri\", StringType()) \\\n",
    "                            .add(\"topic\", StringType()) \\\n",
    "                            .add(\"partition\", IntegerType()) \\\n",
    "                            .add(\"uri\", StringType()) \\\n",
    "                            .add(\"offset\", IntegerType())) \\\n",
    "                         .add(\"minor\",  BooleanType()) \\\n",
    "                         .add(\"namespace\", IntegerType()) \\\n",
    "                         .add(\"parsedcomment\", StringType()) \\\n",
    "                         .add(\"patrolled\", BooleanType()) \\\n",
    "                         .add(\"revision\", StructType() \\\n",
    "                            .add(\"new\", IntegerType()) \\\n",
    "                            .add(\"old\", IntegerType())) \\\n",
    "                         .add(\"server_name\", StringType()) \\\n",
    "                         .add(\"server_script_path\", StringType()) \\\n",
    "                         .add(\"server_url\", StringType()) \\\n",
    "                         .add(\"timestamp\", StringType()) \\\n",
    "                         .add(\"title\", StringType()) \\\n",
    "                         .add(\"type\", StringType()) \\\n",
    "                         .add(\"user\", StringType()) \\\n",
    "                         .add(\"wiki\", StringType()) \\\n",
    "                         .add(\"log_action\", StringType()) \\\n",
    "                         .add(\"log_action_comment\", StringType()) \\\n",
    "                         .add(\"log_id\", IntegerType()) \\\n",
    "                         .add(\"log_params\", StructType()) \\\n",
    "                         .add(\"log_type\", StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the json from the value and cast as string using the schema above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_raw = df.select(from_json(col(\"value\") \\\n",
    "                                .cast(\"string\"), jsonschema) \\\n",
    "                                .alias(\"parsed_wiki_values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[parsed_wiki_values: struct<bot:boolean,comment:string,id:int,length:struct<new:int,old:int>,meta:struct<domain:string,dt:string,id:string,request_id:string,schema_uri:string,topic:string,partition:int,uri:string,offset:int>,minor:boolean,namespace:int,parsedcomment:string,patrolled:boolean,revision:struct<new:int,old:int>,server_name:string,server_script_path:string,server_url:string,timestamp:string,title:string,type:string,user:string,wiki:string,log_action:string,log_action_comment:string,log_id:int,log_params:struct<>,log_type:string>]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_raw.printSchema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Filter raw dataframe and only get English Wikipedia events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_wiki = prune_event(wiki_raw, \"en.wikipedia.org\") \\\n",
    "                        .withColumn(\"anonymous\", matchAnonUDF(\"user\")) \\\n",
    "                        .withColumn(\"timestamp_dt\", from_unixtime(\"timestamp\", \"yyyy-MM-dd HH:mm:ss.SSS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[id: int, user: string, timestamp: string, bot: boolean, comment: string, server_name: string, wiki: string, title: string, type: string, log_action: string, log_action_comment: string, log_type: string, minor: boolean, namespace: int, parsedcomment: string, patrolled: boolean, event_date: string, event_schema_uri: string, wikipage_uri: string, event_domain: string, len_old: int, len_new: int, rev_old: int, rev_new: int, anonymous: boolean, timestamp_dt: string]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_wiki.printSchema "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Filter dataframe and only get Wikidata events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikidata = prune_event(wiki_raw, \"www.wikidata.org\") \\\n",
    "                        .withColumn(\"anonymous\", matchAnonUDF(\"user\")) \\\n",
    "                        .withColumn(\"timestamp_dt\", from_unixtime(\"timestamp\", \"yyyy-MM-dd HH:mm:ss.SSS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[id: int, user: string, timestamp: string, bot: boolean, comment: string, server_name: string, wiki: string, title: string, type: string, log_action: string, log_action_comment: string, log_type: string, minor: boolean, namespace: int, parsedcomment: string, patrolled: boolean, event_date: string, event_schema_uri: string, wikipage_uri: string, event_domain: string, len_old: int, len_new: int, rev_old: int, rev_new: int, anonymous: boolean, timestamp_dt: string]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikidata.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Write to console for 10 second to verify we have a stream as data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enWikiQuery = en_wiki.writeStream.outputMode(\"append\").format(\"console\") \\\n",
    "                                             .start()\n",
    "enWikiQuery.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create windows for English Wikipedia and Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowedEnWikiCounts = windowed_counts(en_wiki, 'timestamp_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowedWikidataCounts = windowed_counts(wikidata, 'timestamp_dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Write the window counts each to their own kafka topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wikidataCounts\n",
    "\n",
    "streamQuery1 = windowedWikidataCounts.select(\n",
    "    to_json(struct(\"window\")).alias(\"key\"),\n",
    "    to_json(struct(\"window\",\"user\", \"count\")).alias(\"value\")) \\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox.hortonworks.com:6667\") \\\n",
    "    .option(\"topic\", \"wikiDataCounts\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://sandbox.hortonworks.com:8020/tmp/wikiDataCounts\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "streamQuery1.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enWikiCounts\n",
    "\n",
    "streamQuery2 = windowedEnWikiCounts.select(\n",
    "    to_json(struct(\"window\")).alias(\"key\"),\n",
    "    to_json(struct(\"window\",\"user\",\"count\")).alias(\"value\")) \\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox.hortonworks.com:6667\") \\\n",
    "    .option(\"topic\", \"enWikiCounts\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://sandbox.hortonworks.com:8020/tmp/enWikiCounts\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "streamQuery2.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  9. Write streams each to their own location on HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_en_wiki_hdfs = write_to_hdfs(en_wiki, \"en_wiki\")\n",
    "query_en_wiki_hdfs.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_wikidata_hdfs = write_to_hdfs(wikidata, \"wikidata\")\n",
    "query_wikidata_hdfs.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Write anonymous english wikipedia entries to a topic called anon-en-wiki-hydrate to notify it's time to add onto that with raw change text and push to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamQuery3 = en_wiki.where(col(\"anonymous\")==True).select(\n",
    "    to_json(struct(\"id\")).alias(\"key\"), \n",
    "    to_json(struct([col(c).alias(c) for c in en_wiki.columns])).alias(\"value\")) \\\n",
    "    .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"sandbox.hortonworks.com:6667\") \\\n",
    "    .option(\"topic\", \"anon-en-wiki-hydrate\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://sandbox.hortonworks.com:8020/tmp/anon-en-wiki-hydrate\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "streamQuery3.awaitTermination(timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Read from anon-en-wiki hydrate and try to grab the raw html diff for later parsing and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hydrate_stream = spark.readStream.format(\"kafka\") \\\n",
    "                     .option(\"kafka.bootstrap.servers\",\"sandbox.hortonworks.com:6667\") \\\n",
    "                     .option(\"subscribe\", \"anon-en-wiki-hydrate\") \\\n",
    "                     .option(\"startingOffsets\", \"earliest\") \\\n",
    "                     .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prunedSchema = StructType().add(\"bot\", BooleanType()) \\\n",
    "\t\t\t\t\t\t   .add(\"comment\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"id\", IntegerType()) \\\n",
    "\t\t\t\t\t\t   .add(\"len_old\", IntegerType()) \\\n",
    "\t\t\t\t\t\t   .add(\"len_new\", IntegerType()) \\\n",
    "\t\t\t\t\t\t   .add(\"rev_new\", IntegerType()) \\\n",
    "\t\t\t\t\t\t   .add(\"rev_old\", IntegerType()) \\\n",
    "\t\t\t\t\t\t   .add(\"parsedcomment\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"patrolled\", BooleanType()) \\\n",
    "\t\t\t\t\t\t   .add(\"title\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"type\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"user\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"wiki\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"log_action\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"log_action_comment\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"log_id\", IntegerType()) \\\n",
    "\t\t\t\t\t\t   .add(\"log_type\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"server_name\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"timestamp\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"timestamp_dt\", DateType()) \\\n",
    "\t\t\t\t\t\t   .add(\"minor\",  BooleanType()) \\\n",
    "\t\t\t\t\t\t   .add(\"anonymous\", BooleanType()) \\\n",
    "\t\t\t\t\t\t   .add(\"event_date\", DateType()) \\\n",
    "\t\t\t\t\t\t   .add(\"event_schema_uri\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"wikipage_uri\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"event_domain\", StringType()) \\\n",
    "\t\t\t\t\t\t   .add(\"namespace\", IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note there might be an issue with how these values are coming back, i noticed far more null values coming back than expected\n",
    "# even though confirming that values going into stream look alright.\n",
    "hydrate_raw = hydrate_stream.select(from_json(col(\"value\") \\\n",
    "                                .cast(\"string\"), prunedSchema) \\\n",
    "                                .alias(\"hydrate_values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hydrate_df = hydrate_raw.select(\"hydrate_values.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydrateQuery = hydrate_df.writeStream.outputMode(\"append\").format(\"console\") \\\n",
    "                                             .start()\n",
    "hydrateQuery.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrated_df = hydrate_df.withColumn(\"change_text\", rawChangesUDF(\"rev_old\", \"rev_new\")) \\\n",
    "                        .where(col(\"rev_old\") != None) \\\n",
    "                        .where(col(\"rev_new\") != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrated_df_to_hdfs = hydrated_df.where(col(\"change_text\") != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_hydrated_df_hdfs = write_to_hdfs(hydrated_df_to_hdfs, \"hydrated_en_wiki\")\n",
    "query_hydrated_df_hdfs.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In theory the last part should work to grab any records and add in a field \"change_text\" that contains the HTML string which is the text diff between wikipedia changes for English language wikipedia. Though it is fairly untested how well it might perform on a running stream. To be conservative, it just bails if a scrape did not work and only records with found changes are sent to the HDFS store. \n",
    "\n",
    "Were there more time, I would probably test this further and also try to tweak things on the kafka side to make sure that change texts were more reliably being grabbed and stored.\n",
    "\n",
    "Overall, I think maybe I bit off more than I could chew and it might have been good just to focus on this one aspect of getting English Wikipedia changes with their original change text into a storage point for later analysis and not worry too much about the windowed bot count.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
