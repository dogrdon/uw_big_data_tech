{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import * # for structured data types\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql.functions import from_json, col, avg, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from simulated temerature stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\",\"sandbox.hortonworks.com:6667\") \\\n",
    "    .option(\"subscribe\", \"tempstream\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack Kafka binary key and value to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see that we have events streaming in by emitting to console\n",
    "\n",
    "Setting a timeout as I don't know how we'd interrupt the query stream in jupyter since next cell won't run till this one finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = data.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination(timeout=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse to json, first we want to define the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"server\" : \"c14-5c2s7\", \n",
    "#  \"timestamp\" : \"2018-03-03 19:31:39\", \n",
    "#  \"sensors\" : {\n",
    "#        \"sensor1\" : , \n",
    "#        \"sensor2\" : , \n",
    "#        \"sensor3\" : , \n",
    "#        \"sensor4\" : , \n",
    "#        \"sensor5\" : , \n",
    "#        \"sensor6\" : , \n",
    "#        \"sensor7\" : , \n",
    "#        \"sensor8\" : ,\n",
    "#        \"sensor9\" : , \n",
    "#        \"sensor10\" : \n",
    "#}}\n",
    "\n",
    "jsonschema = StructType().add(\"server\", StringType()) \\\n",
    "                     .add(\"timestamp\", StringType()) \\\n",
    "                     .add(\"sensors\", StructType() \\\n",
    "                          .add(\"sensor1\", IntegerType()) \\\n",
    "                          .add(\"sensor2\", IntegerType()) \\\n",
    "                          .add(\"sensor3\", IntegerType()) \\\n",
    "                          .add(\"sensor4\", IntegerType()) \\\n",
    "                          .add(\"sensor5\", IntegerType()) \\\n",
    "                          .add(\"sensor6\", IntegerType()) \\\n",
    "                          .add(\"sensor7\", IntegerType()) \\\n",
    "                          .add(\"sensor8\", IntegerType()) \\\n",
    "                          .add(\"sensor9\", IntegerType()) \\\n",
    "                          .add(\"sensor10\", IntegerType()) \n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then parse the json using `from_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df.select(from_json(col(\"value\").cast(\"string\"), jsonschema).alias(\"parsed_value\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab everything in the json for a dataframe of all, raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_data.select(\"parsed_value.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab all invalid records (any where at least one sensor is a 0 or negative value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invalid = df_data.select(\"parsed_value.*\").where((col('sensors.sensor1') < 1)|\n",
    "                                               (col('sensors.sensor2') < 1)|\n",
    "                                               (col('sensors.sensor3') < 1)|\n",
    "                                               (col('sensors.sensor4') < 1)|\n",
    "                                               (col('sensors.sensor5') < 1)|\n",
    "                                               (col('sensors.sensor6') < 1)|\n",
    "                                               (col('sensors.sensor7') < 1)|\n",
    "                                               (col('sensors.sensor8') < 1)|\n",
    "                                               (col('sensors.sensor9') < 1)|\n",
    "                                               (col('sensors.sensor10') < 1)\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just run a few queries to see parsed items in console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = df_raw.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query1.awaitTermination(timeout=10) #set a timeout as I don't know how we'd interrupt the query stream in jupyter since next cell won't run till this one finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = df_invalid.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query2.awaitTermination(timeout=10) #set a timeout as I don't know how we'd interrupt the query stream in jupyter since next cell won't run till this one finishes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write raw and invalid data as parquet into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = df_raw.writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"path\", \"hdfs://sandbox.hortonworks.com:8020/tmp/temp_raw\") \\\n",
    "  .option(\"checkpointLocation\", \"hdfs://sandbox.hortonworks.com:8020/tmp/temp_raw\") \\\n",
    "  .start()\n",
    "\n",
    "query3.awaitTermination(timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = df_invalid.writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"path\", \"hdfs://sandbox.hortonworks.com:8020/tmp/temp_invalid\") \\\n",
    "  .option(\"checkpointLocation\", \"hdfs://sandbox.hortonworks.com:8020/tmp/temp_invalid\") \\\n",
    "  .start()\n",
    "\n",
    "query4.awaitTermination(timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just pull form the HDFS store to make sure we got records there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_raw = sqlContext.read.parquet('hdfs://sandbox.hortonworks.com:8020/tmp/temp_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_invalid = sqlContext.read.parquet('hdfs://sandbox.hortonworks.com:8020/tmp/temp_invalid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate running averages for Sensors 1 & 2 and make sure the column name for the dataframe is value so we can stream to Kafka topics\n",
    "\n",
    "(Also just stream to console quickly to make sure it worked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor1_avg = df_data.select(\"parsed_value.*\").select(avg(when((col('sensors.sensor1') > 1), col('sensors.sensor1'))).cast(\"string\").alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = sensor1_avg.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "query5.awaitTermination(timeout=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor2_avg = df_data.select(\"parsed_value.*\").select(avg(when((col('sensors.sensor2') > 1), col('sensors.sensor2'))).cast(\"string\").alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query6 = sensor2_avg.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "query6.awaitTermination(timeout=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1 - Write averages to another Kafka topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might not work due to bug mentioned in assignment, though I might also be sending to the topic incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query8 = sensor1_avg.writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"sandbox.hortonworks.com:6667\") \\\n",
    "  .option(\"topic\", \"avg1\") \\\n",
    "  .option(\"checkpointLocation\", \"hdfs://sandbox.hortonworks.com:8020/tmp/avg1_chkpt\") \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .start()\n",
    "    \n",
    "query8.awaitTermination(timeout=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query8 = sensor2_avg.writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"sandbox.hortonworks.com:6667\") \\\n",
    "  .option(\"topic\", \"avg2\") \\\n",
    "  .option(\"checkpointLocation\", \"hdfs://sandbox.hortonworks.com:8020/tmp/avg2_chkpt\") \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .start()\n",
    "    \n",
    "query8.awaitTermination(timeout=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
